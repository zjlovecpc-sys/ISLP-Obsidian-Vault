---
tags:
  - 统计学习
  - 备赛资料
  - 速查表
aliases:
  - 名词一览表
  - Cheatsheet
---
# 统计学习核心名词速查表

这份速查表是美赛建模的“词典”，涵盖了从回归基础到分类进阶的所有核心概念。

## 一、 基础建模与参数估计 (回归篇)

| **核心名词**                                     | **核心公式 / 符号**                               | **名词解释**                                                  | **对应笔记**                              |
| -------------------------------------------- | ------------------------------------------- | --------------------------------------------------------- | ------------------------------------- |
| **[[简单线性回归]]**<br>(Simple Linear Regression) | $Y \approx \beta_0 + \beta_1 X$             | 假设 $X$ 和 $Y$ 之间是一条直线的关系，是所有回归分析的基础。                       | [[简单线性回归#核心定义]]                       |
| **外生变量**<br>(Exogenous Variable)             | `exog` (代码参数)                               | 即 **自变量 $X$** / 特征。在 `statsmodels` 库中，输入特征矩阵通常被称为 `exog`。 | [[Statsmodels与Sklearn对比]]             |
| **总体回归线**<br>(Population Regression Line)    | $Y = \beta_0 + \beta_1 X + \epsilon$        | 变量间的“真实”数学关系，包含不可观测的真实参数和随机误差 $\epsilon$。我们永远无法直接观测到它。    | [[简单线性回归#总体回归线 vs 最小二乘线]]             |
| **系数/参数**<br>(Coefficients)                  | $\beta_0$ (截距), $\beta_1$ (斜率)              | **$\beta_1$**：$X$ 每增加 1 个单位，$Y$ 平均增加的数量。是模型需要估计的核心目标。     | [[简单线性回归#核心定义]]                       |
| **预测值 / 拟合值**<br>(Fitted Value)              | $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ | 将特定数值代入已估算参数的模型后计算出的 $Y$ 值（$\hat{}$ 符号代表估算值）。             | [[简单线性回归]]                            |
| **残差**<br>(Residual)                         | $e_i = y_i - \hat{y}_i$                     | **真实值 - 预测值**。表示模型没猜准的部分，即样本点到拟合回归线的垂直距离。                 | [[简单线性回归#参数估计：最小二乘法 (Least Squares)]] |
| **残差平方和**<br>(RSS)                           | $\text{RSS} = \sum e_i^2$                   | 所有观测点预测误差平方的总和。**它是[[最小二乘法]]优化的目标函数**。                    | [[简单线性回归#参数估计：最小二乘法 (Least Squares)]] |
| **[[最小二乘法]]**<br>(Least Squares)             | $\min(\text{RSS})$                          | 一种数学优化技术，通过最小化残差平方和来寻找回归线的最佳位置。                           | [[最小二乘法#核心定义]]                        |

---

## 二、 评估模型精度 (回归篇)

| **核心名词**                          | **核心公式 / 符号**                         | **名词解释**                                      | **对应笔记**                                 |
| --------------------------------- | ------------------------------------- | --------------------------------------------- | ---------------------------------------- |
| **无偏估计**<br>(Unbiased)            | $E(\hat{\mu}) = \mu$                  | 虽单次估计有误差，但只要重复实验次数足够多，估计值的期望等于真实值。            | [[模型参数估计与检验]]                            |
| **标准误**<br>(SE, Standard Error)   | $\text{SE}(\hat{\beta}_1)$            | 衡量系数估计值的变异性。SE 越小，说明估计值在不同样本间越稳定。             | [[模型参数估计与检验#标准误 (Standard Error, SE)]]   |
| **残差标准误**<br>(RSE)                | $\sqrt{\frac{\text{RSS}}{n-p-1}}$     | **误差的物理标尺**。衡量模型预测平均偏离真实数据多少个原始单位。            | [[回归模型评估指标#1. 残差标准误 (RSE)]]              |
| **置信区间**<br>(Confidence Interval) | $\hat{\beta}_1 \pm 2 \cdot \text{SE}$ | 真实系数以 95% 的概率落在此范围内。若区间不包含 0，则系数显著。           | [[模型参数估计与检验#置信区间 (Confidence Interval)]] |
| **预测区间**<br>(Prediction Interval) | -                                     | **个体预测范围**。包含随机误差 $\epsilon$，范围比预测均值的置信区间宽很多。 | [[预测区间与置信区间]]                            |
| **t 统计量**<br>(t-statistic)        | $t = \frac{\hat{\beta}_1}{SE}$        | 用于检验变量显著性。衡量系数偏离 0 有多少个标准误，绝对值越大越显著。          | [[模型参数估计与检验#t统计量与p值]]                    |
| **p 值**<br>(p-value)              | $P(>\|t\|)$                           | **显著性判定指标**。原假设（系数为0）成立时观测到当前结果的概率。           | [[模型参数估计与检验#t统计量与p值]]                    |
| **总平方和**<br>(TSS)                 | $\sum (y_i - \bar{y})^2$              | 响应变量 $Y$ 自身的总变异量，即不使用任何自变量预测时的基准误差。           | [[回归模型评估指标#2. $R^2$ 统计量 (R-squared)]]    |
| **$R^2$ 统计量**<br>(R-squared)      | $1 - \frac{\text{RSS}}{\text{TSS}}$   | 模型解释了数据总变动量的比例。反映模型对观测值的拟合优度。                 | [[回归模型评估指标#2. $R^2$ 统计量 (R-squared)]]    |

---

## 三、 多元回归与预测 (回归篇)

| **核心名词**                                       | **核心公式 / 符号**                     | **名词解释**                                            | **对应笔记**                             |
| ---------------------------------------------- | --------------------------------- | --------------------------------------------------- | ------------------------------------ |
| **[[多元线性回归]]**<br>(Multiple Linear Regression) | $Y = \sum \beta_j X_j + \epsilon$ | 引入多个自变量。$\beta_j$ 表示：**固定其他变量不变**，$X_j$ 对 $Y$ 的净影响。 | [[多元线性回归]]                           |
| **F 统计量**<br>(F-statistic)                     | -                                 | **集体检阅**。t 值看单个变量，F 值看**这一堆变量里是否至少有一个是有用的**。        | [[回归模型评估指标#3. F统计量 (F-statistic)]]   |
| **变量选择**<br>(Variable Selection)               | 向前/向后/混合选择                        | 变量太多时的“试错法”，旨在剔除垃圾变量，保留精华。                          | [[多元线性回归#变量选择 (Variable Selection)]] |
| **预测区间**<br>(Prediction Interval)              | 宽于置信区间                            | 预测**某一个特定点**的值（包含个体误差 $\epsilon$），所以范围比预测平均值更宽。     | [[预测区间与置信区间]]                        |

---

## 四、 扩展模型与诊断 (回归篇)

| **核心名词**                             | **核心公式 / 符号**    | **名词解释**                                     | **对应笔记**                                     |
| ------------------------------------ | ---------------- | -------------------------------------------- | -------------------------------------------- |
| **虚拟变量**<br>(Dummy Variable)         | 0 或 1            | 将定性变量（如性别、类别）转换为数值，以便计算机进行回归运算。              | [[多元线性回归#处理定性变量 (Qualitative Predictors)]]   |
| **交互效应**<br>(Interaction Effect)     | $X_1 \times X_2$ | **1+1>2**。一个变量对 $Y$ 的效果取决于另一个变量的取值水平。        | [[回归扩展模型#1. 交互效应 (Interaction Effect)]]      |
| **层级原则**<br>(Hierarchical Principle) | -                | **统计约束**。模型中若包含交互项，则其对应的主效应项必须保留，即使不显著。      | [[回归扩展模型#1. 交互效应 (Interaction Effect)]]      |
| **多项式回归**<br>(Polynomial Regression) | $X^2, X^3$       | 引入自变量的高阶项以拟合非线性趋势。                           | [[回归扩展模型#2. 多项式回归 (Polynomial Regression)]]  |
| **非线性**<br>(Non-linearity)           | 残差呈 U 形          | **诊断指标**。若残差图有明显规律趋势，说明线性假设不成立，需引入非线性项。      | [[回归诊断与潜在问题#1. 非线性 (Non-linearity)]]         |
| **异方差性**<br>(Heteroscedasticity)     | 残差呈漏斗形           | **方差不齐**。预测值越大，残差波动越大。解法：对 $Y$ 进行 $\log$ 变换。 | [[回归诊断与潜在问题#3. 异方差性 (Heteroscedasticity)]]   |
| **离群点**<br>(Outliers)                | 学生化残差 $> 3$      | **$Y$ 值离谱**。观测值远离模型拟合线，对模型评价指标（如 $R^2$）影响显著。 | [[回归诊断与潜在问题#4. 离群点 (Outliers)]]              |
| **高杠杆点**<br>(High Leverage Points)   | $h_i$ 统计量        | **$X$ 值离谱**。位于数据分布边缘，具有极强的“撬动”回归线走向的能力。      | [[回归诊断与潜在问题#5. 高杠杆点 (High Leverage Points)]] |
| **[[共线性与VIF]]**<br>(Collinearity)    | $VIF > 5$ 或 $10$ | 变量间高度相关。导致分不清变量贡献，系数估计不稳定且标准误增大。             | [[共线性与VIF#诊断方法]]                             |

---

## 五、 分类算法 (Classification)

| **核心名词**                                               | **核心公式 / 符号**                              | **名词解释**                                          | **对应笔记**                                 |
| ------------------------------------------------------ | ------------------------------------------ | ------------------------------------------------- | ---------------------------------------- |
| **[[逻辑回归]]** <br><br>(Logistic Regression)             | $p(X) = \frac{e^{\beta X}}{1+e^{\beta X}}$ | 用于二分类问题的非线性回归模型，通过 Logistic 函数将输出映射为概率 $[0, 1]$。  | [[逻辑回归]]                                 |
| **几率**<br>(Odds)                                       | $\frac{p}{1-p}$                            | 事件发生的概率与不发生概率的比值。几率越大，代表事件发生的可能性越高。               | [[逻辑回归#关键变换：几率与对数几率]]                    |
| **对数几率**<br>(Logit)                                    | $\log(\frac{p}{1-p})$                      | **核心变换**。几率的对数，它与自变量 $X$ 呈线性关系，是逻辑回归的链接函数。        | [[逻辑回归#关键变换：几率与对数几率]]                    |
| **极大似然估计**<br>(MLE)                                    | $\max L(\beta)$                            | **参数估计准则**。寻找一组参数，使得观测到当前这组样本数据的可能性（似然性）达到最大。     | [[逻辑回归#参数估计：极大似然法 (Maximum Likelihood)]] |
| **[[线性判别分析LDA]]**<br>(Linear Discriminant Analysis)    | $\Sigma_k = \Sigma$                        | **核心假设**：假设各类数据服从多元正态分布且共享相同的协方差矩阵。产生线性决策边界。      | [[线性判别分析LDA]]                            |
| **[[二次判别分析QDA]]**<br>(Quadratic Discriminant Analysis) | $\Sigma_k \neq \Sigma$                     | **核心假设**：各类别拥有独立的协方差矩阵。决策边界是 $X$ 的二次函数，比 LDA 更灵活。 | [[二次判别分析QDA]]                            |
| **[[朴素贝叶斯]]**<br>(Naive Bayes)                         | $f_k(x) = \prod f_{kj}(x_j)$               | **独立性假设**：假设在给定类别的条件下，各特征之间相互独立。极大地减少了参数估计量。      | [[朴素贝叶斯]]                                |
| **[[K最近邻法]]**<br>(K-Nearest Neighbors)                 | -                                          | **非参数方法**。通过寻找 $X$ 空间中最近的 $K$ 个点，按“少数服从多数”原则进行分类。 | [[K最近邻法]]                                |
| **混淆现象**<br>(Confounding)                              | 辛普森悖论                                      | **分析陷阱**。在多元逻辑回归中，由于遗漏关键变量导致原变量系数符号发生反转或显著性消失。    | [[多元逻辑回归与混淆现象]]                          |

---

## 六、 模型评估与工程规范 (分类篇)

| **核心名词**                                | **核心公式 / 符号**            | **名词解释**                                                            | **对应笔记**                                          |
| --------------------------------------- | ------------------------ | ------------------------------------------------------------------- | ------------------------------------------------- |
| **训练/测试集切分**<br>(Train/Test Split)      | -                        | **工程规范**。将数据划分为训练集（估计参数）和测试集（评估泛化能力），是防止过拟合的基础。                     | [[KNN_sklearn实现#2. 训练与测试拆分]]                      |
| **标准化**<br>(Standardization)            | $\frac{x - \mu}{\sigma}$ | **预处理必备**。消除不同特征间量纲的影响（如工资与年龄），防止大数值特征主导距离计算。                       | [[数据标准化]]                                         |
| **混淆矩阵**<br>(Confusion Matrix)          | TP, TN, FP, FN           | **评估核心**。展示预测类别与真实类别的交叉频数表。是计算各项分类指标的基石。                            | [[混淆矩阵与ROC#2. 混淆矩阵 (Confusion Matrix)]]           |
| **准确率**<br>(Accuracy)                   | $\frac{TP+TN}{Total}$    | 模型预测正确的样本占总样本的比例。在不平衡数据（如罕见病检测）下往往失效。                               | [[混淆矩阵与ROC#1. 为什么准确率 (Accuracy) 不够？]]             |
| **查准率**(Precision)                      | $\frac{TP}{TP+FP}$       | **精确率**。在所有被预测为阳性的样本中，实际为阳性的比例。侧重于“防虚警”。                            | [[混淆矩阵与ROC#核心指标]]                                 |
| **敏感度 / 召回率**<br>(Sensitivity / Recall) | $\frac{TP}{TP+FN}$       | **召回率**。在所有真实为阳性的样本中，被正确找出的比例。侧重于“防漏诊”。                             | [[混淆矩阵与ROC#核心指标]]                                 |
| **特异度**<br>(Specificity)                | $\frac{TN}{TN+FP}$       | **防误诊指标**。在所有真实为阴性的样本中，被正确预测为阴性的比例。                                 | [[混淆矩阵与ROC#核心指标]]                                 |
| **ROC 曲线**<br>(ROC Curve)               | TPR vs FPR               | 以真阳性率为纵轴，假阳性率为横轴。展示不同分类阈值下模型的性能轨迹。                                  | [[混淆矩阵与ROC#3. ROC 曲线与 AUC]]                       |
| **AUC**<br>(Area Under Curve)           | -                        | ROC 曲线下的面积。衡量分类器在所有可能阈值下的综合性能，数值越接近 1 越好。                           | [[混淆矩阵与ROC#3. ROC 曲线与 AUC]]                       |
| **偏差**<br>(Deviance)                    | -                        | **GLM 版 RSS**。衡量观测值与饱和模型间的差异。Residual Deviance 越小说明模型拟合越好。          | [[GLM评估指标#1. 偏差 (Deviance)]]                      |
| **AIC**<br>(Akaike Info Criterion)      | $-2\log(L) + 2p$         | **模型选择准则**。同时考虑模型拟合优度和复杂度（变量数 $p$）。AIC 越小模型越理想。                     | [[GLM评估指标#2. 信息准则 (AIC / BIC)]]                   |
| **[[验证集方法]]**<br>(Validation Set)       | -                        | **最简重采样**。将数据随机分为训练集和验证集。缺点是结果波动大（高方差）且浪费数据（高偏差）。                   | [[验证集方法]]                                         |
| **[[留一法交叉验证]]**<br>(LOOCV)              | $n$ 次模型拟合                | **极端交叉验证**。每次只留 1 个样本做测试。无随机性，偏差极低，但计算成本高。                          | [[留一法交叉验证]]                                       |
| **[[K折交叉验证]]**<br>(K-Fold CV)           | $K=5$ or $10$            | **黄金标准**。将数据分 K 份轮流验证。是偏差与方差的最佳平衡点 (Sweet Spot)。                    | [[K折交叉验证]]                                        |
| **[[自助法]]**<br>(Bootstrap)              | 有放回抽样                    | **参数估计神器**。通过对原始数据进行重复的有放回抽样，来估计复杂统计量的标准误或置信区间。                     | [[自助法]]                                           |
| **OOB 样本**<br>(Out-of-Bag)              | $1/e \approx 36.8\%$     | **袋外数据**。在自助法抽样中未被选中的那部分数据（约占 1/3），常用于模型验证。                         | [[自助法#样本覆盖率与袋外 (OOB) 观测]]                         |
| **计算捷径**<br>(Shortcut Formula)          | $\frac{1}{1-h_i}$        | **魔法公式**。仅适用于最小二乘回归，通过杠杆值 $h_i$ 在一次拟合中算出 LOOCV 误差。                  | [[留一法交叉验证#💡 计算捷径与解析性质 (Computational Shortcut)]] |
| **ShuffleSplit**                        | **sklearn**              | **随机拆分器。与 KFold 不同，它允许重复抽样，且训练集/测试集大小与迭代次数完全解耦。适合海量数据的蒙特卡洛验证。**     | **[[交叉验证实战#5. 进阶：蒙特卡洛交叉验证 (Repeated CV)]]**       |
| **sklearn_sm**                          | `ISLP`                   | **适配器**。一个将 `statsmodels` 模型包装成 `sklearn` 格式的工具，打通了统计推断与机器学习工作流的壁垒。 | [[交叉验证实战#0. 核心工具库]]                               |

## 七、 广义线性模型 (GLM) 家族

| **核心名词**                              | **适用场景**                                          | **关键假设 (方差)**                                                 | **链接函数 (Link)**       | **对应笔记**    |
| ------------------------------------- | ------------------------------------------------- | ------------------------------------------------------------- | --------------------- | ----------- |
| **[[泊松回归]]**<br>(Poisson Regression)  | **计数数据**。预测一段时间或空间内某事件发生的次数（如点击量）。                | **均值等于方差** ($Var = \mu$)。若方差远大于均值则不适用。                        | $\log(\mu)$           | [[泊松回归]]    |
| **[[负二项回归]]**<br>(Negative Binomial)  | **过离散计数**。当计数数据的波动极大，不满足泊松回归假设时使用。                | **方差大于均值** ($Var = \mu + \mu^2/\theta$)。引入辅助参数 $\theta$ 吸收噪声。 | $\log(\mu)$           | [[负二项回归]]   |
| **[[Gamma回归]]**<br>(Gamma Regression) | **连续正值且右偏**。常用于保险理赔金额、降雨量、等待时间建模。                 | **方差随均值的平方增长** ($Var \propto \mu^2$)。对大数值波动更具容忍度。             | $\log(\mu)$ 或 $1/\mu$ | [[Gamma回归]] |
| **[[逆高斯回归]]**<br>(Inverse Gaussian)   | **极端右偏长尾**。适用于有极个别超大数值的场景，如设备寿命、金融灾难损失。           | **方差随均值的立方增长** ($Var \propto \mu^3$)。比 Gamma 分布更“重尾”。         | $1/\mu^2$             | [[逆高斯回归]]   |
| **[[GLM链接函数]]**<br>(Link Function)    | **数学桥梁**。将不受限的线性预测值 $X\beta$ 映射到符合物理约束（如非负）的均值空间。 | 保证模型输出在逻辑和物理意义上是合理的（如概率必须在 0-1 之间）。                           | $g(\mu) = X\beta$     | [[GLM链接函数]] |

## 八、 线性模型选择与正则化 (进阶回归)
| **核心名词**                                               | **核心公式 / 符号**                               | **名词解释**                                                                 | **对应笔记**                                                 |
| ------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------- |
| **[[子集选择法]]**<br>(Subset Selection)                    | 最优 / 前向 / 后向                                | **特征筛选**。从 $p$ 个变量中选出一个最佳子集来拟合模型。最优子集法需遍历 $2^p$ 个模型，计算量最大。               | [[子集选择法]]                                                |
| **$C_p$ / AIC / BIC**                                  | 惩罚项 $\propto d$                             | **间接评估指标**。通过给训练误差（RSS）加上一个与模型复杂度（$d$）成正比的惩罚项，来估计测试误差。                   | [[模型选择指标]]                                               |
| **调整 $R^2$**<br>(Adjusted $R^2$)                       | $1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$         | **修正版 $R^2$**。只有当新加入的变量对模型的贡献超过了随机噪声时，它的值才会上升。                           | [[模型选择指标#4. 调整 $R^2$ (Adjusted $R^2$)]]                  |
| **[[L1与L2范数#1. L1 范数 (L1 Norm)\|L1 范数]]**<br>(L1 Norm) | $\|\beta\|_1 = \sum \|\beta_j\|$            | **曼哈顿距离**。几何形状为菱形，有尖角。容易产生稀疏解（系数为0）。核心用于 [[套索回归]]。                       | [[L1与L2范数]]                                              |
| **[[L1与L2范数#2. L2 范数 (L2 Norm)\|L2 范数]]**<br>(L2 Norm) | $\|\beta\|_2 = \sqrt{\sum \beta_j^2}$       | **欧几里得距离**。几何形状为圆形，光滑。仅收缩系数但不为0。核心用于 [[岭回归]]。                            | [[L1与L2范数]]                                              |
| **[[岭回归]]**<br>(Ridge Regression)                      | $\lambda \sum \beta_j^2$ (L2)               | **L2 正则化**。通过引入偏差来大幅降低方差。系数会收缩向 0 但**永远不为 0**。必须先标准化数据。                  | [[岭回归]]                                                  |
| **[[套索回归]]**<br>(The Lasso)                            | $\lambda \sum \|\beta_j$ (L1)               | **L1 正则化**。具有**稀疏性**（Sparsity），能将不重要的变量系数**压缩为 0**，实现自动特征选择。             | [[套索回归]]                                                 |
| **调节参数**<br>(Tuning Parameter)                         | $\lambda$                                   | **控制阀门**。$\lambda=0$ 时等同于最小二乘法；$\lambda \to \infty$ 时所有系数趋于 0。需通过交叉验证确定。 | [[正则化调参指南]]                                              |
| **一倍标准误准则**<br>(1-SE Rule)                             | $CV_{min} + SE$                             | **选简原则**。不选 CV 误差最小的模型，而是在其 1 倍标准误范围内，选择**最简单**（变量最少）的模型。                | [[正则化调参指南#一倍标准误法则 (One-Standard-Error Rule)]]            |
| **[[降维方法]]**<br>(Dimension Reduction)                  | $M < p$                                     | **空间压缩**。将原始的 $p$ 个变量转换为 $M$ 个线性组合（如主成分），再进行回归。                          | [[降维方法]]                                                 |
| **PCR**<br>(Principal Components Regression)           | PCA + OLS                                   | **主成分回归**。利用 PCA 提取方差最大的方向作为新特征。假设“变化大”的方向就是“与 Y 相关”的方向。                 | [[降维方法#1. 主成分回归 (Principal Components Regression, PCR)]] |
| **维度灾难**<br>(Curse of Dimensionality)                  | $p > n$                                     | **高维困境**。当变量数超过样本量时，标准最小二乘法失效（方程组有无穷解）。必须使用正则化或降维。                       | [[高维数据问题]]                                               |
| **$C_p$ 统计量**<br>($C_p$ Statistic)                     | $C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)$ | 衡量模型测试误差的无偏估计。越小越好。惩罚项为 $2d\hat{\sigma}^2$。                              | [[模型选择指标]]                                               |
| 贝叶斯信息准则<br>(BIC)                                       | $\frac{1}{n}(RSS + \log(n)d\hat{\sigma}^2)$ | 相比 $C_p$，BIC 对模型复杂度（$d$）的惩罚更重（因为 $\log(n) > 2$ 当 $n>7$）。倾向于选更简单的模型。      | [[模型选择指标]]                                               |
| 调整 $R^2$<br>(Adjusted $R^2$)                           | $1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}$         | 唯一一个**越大越好**的特征选择指标。修正了普通 $R^2$ 随变量增加只增不减的缺陷。                            | [[模型选择指标]]                                               |