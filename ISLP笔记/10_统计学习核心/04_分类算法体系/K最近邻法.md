---
tags:
  - 统计学习
  - 分类算法
  - 非参数方法
  - Python算法
aliases:
  - KNN
  - K-Nearest Neighbors
---

# K最近邻法 (KNN)

## 核心原理
**KNN** 是一种常用的[[参数与非参数方法#非参数方法 (Non-parametric Methods)|非参数方法]]，常用于分类（也可用于回归）。

对于一个新的观测点 $x_0$，KNN 算法步骤如下：
1.  在训练集中找到距离 $x_0$ 最近的 $K$ 个点，记为集合 $\mathcal{N}_0$。
2.  估计 $x_0$ 属于类别 $j$ 的概率：即计算这 $K$ 个点中有多少比例属于类别 $j$。
    $$P(Y=j|X=x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)$$
3.  使用[[贝叶斯分类器#核心定义|贝叶斯分类器]]规则，将 $x_0$ 分类为概率最大的那一类。

## K值的选择（关键）
- **K 很小 (如 K=1)**：
    - 模型非常灵活（决策边界极其扭曲，甚至把噪声都圈进去了）。
    - 低偏差，高方差（[[偏差-方差权衡#三大成分|过拟合]]）。
- **K 很大 (如 K=100)**：
    - 模型变得迟钝（决策边界接近直线）。
    - 高偏差，低方差（欠拟合）。
- **美赛应用**：通常需要通过交叉验证（Cross-Validation）来选择最优的 $K$ 值。

## 模型评估
- 对于分类问题，我们使用 [[分类错误率]] 来评估 KNN 的表现。
- 我们的目标是找到一个 $K$ 值，使得测试错误率最低（接近 [[贝叶斯分类器#贝叶斯误差率 (Bayes Error Rate)|贝叶斯误差率]]）。
## 代码实现
- **关键步骤**：[[KNN_sklearn实现#1. 数据标准化 (Standardization)]] (不做标准化，KNN 必死)
- 循环调参代码：[[KNN_sklearn实现#4. 选择最佳的 K (调参)]]