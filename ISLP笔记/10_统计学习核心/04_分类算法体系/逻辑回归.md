---
tags:
  - 统计学习
  - 逻辑回归
  - 核心算法
aliases:
  - Logistic Regression
  - Logit
  - Odds
  - 几率
---

# 逻辑回归

## 核心模型
为了解决线性回归预测值越界的问题，**逻辑回归** 使用 **Logistic 函数** (或称 Sigmoid 函数) 对线性输出进行压缩：

$$p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}$$

- **特点**：无论 $\beta$ 和 $X$ 取什么值，输出 $p(X)$ 永远在 $[0, 1]$ 之间，呈现 S 型曲线。

## 关键变换：几率与对数几率
为了让公式变回线性形式，我们做如下变换：

1.  **几率 (Odds)**：
    $$\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X}$$
    - 含义：发生事件的概率与不发生事件概率的比值（赌博中常用）。如果 $p=0.8$，则几率为 $0.8/0.2 = 4$。
2.  **对数几率 (Log-odds / Logit)**：
    $$\log \left( \frac{p(X)}{1-p(X)} \right) = \beta_0 + \beta_1 X$$
    - **结论**：逻辑回归模型对于**对数几率**而言是线性的。

## 系数 $\beta_1$ 的解释
在逻辑回归中，$\beta_1$ 的含义不再是 $Y$ 的直接增量，而是：
- $X$ 每增加一个单位，**对数几率**增加 $\beta_1$。
- 或者说，**几率 (Odds)** 乘以 $e^{\beta_1}$ 倍。

## 关联笔记
- 参数估计方法：见 [[极大似然估计MLE]]。
- 推广到多个变量：见 [[多元逻辑回归与混淆现象]]。

## 代码实现
- Python 核心代码：[[逻辑回归与判别分析实现#2. 逻辑回归 (Statsmodels)]]
- 库的选择对比：[[Statsmodels与Sklearn对比]]