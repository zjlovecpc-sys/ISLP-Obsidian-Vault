---
tags:
  - 统计学习
  - 非线性模型
  - 样条
  - 正则化
aliases:
  - Smoothing Splines
  - 惩罚平方和
  - 有效自由度
  - LOOCV魔法公式
---

# 平滑样条 (Smoothing Splines)

## 1. 核心思想：损失函数 + 惩罚项
与 [[回归样条]] 手动选择节点不同，平滑样条通过最小化一个包含**粗糙度惩罚 (Roughness Penalty)** 的目标函数来自动确定曲线形状：

$$\sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g''(t)^2 dt$$

* **第一项**：残差平方和 (RSS)，衡量**拟合程度**。
* **第二项**：惩罚项。$g''(t)$ 是函数的二阶导数（曲率）。$\int g''(t)^2 dt$ 衡量函数的**总波动程度**。
* **$\lambda$ (平滑参数)**：控制偏差-方差权衡的阀门。

## 2. $\lambda$ 的极端情况
* **$\lambda = 0$**：惩罚项失效。$g(x)$ 可以任意弯曲以穿过所有数据点。$\implies$ **插值样条 (Interpolating Spline)**，过拟合，方差极大。
* **$\lambda \to \infty$**：惩罚项占主导。为了使积分最小，$g''(t)$ 必须处处为 0，即 $g(t)$ 是一条直线。$\implies$ **最小二乘线性回归**，欠拟合，偏差极大。

## 3. 数学性质
虽然我们在所有 $x_i$ 处都设置了节点（看起来参数爆炸），但证明显示，能最小化上述目标函数的 $g(x)$ 必然是一个**自然三次样条 (Natural Cubic Spline)**。

## 4. 有效自由度 (Effective Degrees of Freedom)
在平滑样条中，我们不再数参数个数，而是计算**有效自由度 ($df_{\lambda}$)**。
写作矩阵形式 $\hat{\mathbf{g}}_\lambda = \mathbf{S}_\lambda \mathbf{y}$，其中 $\mathbf{S}_\lambda$ 是 $n \times n$ 的平滑矩阵。
$$df_{\lambda} = \text{trace}(\mathbf{S}_\lambda) = \sum_{i=1}^n \{ \mathbf{S}_\lambda \}_{ii}$$
* $df_{\lambda}$ 随着 $\lambda$ 的增加从 $n$ 减少到 2。

## 5. 编程手必杀技：LOOCV 魔法公式
对于平滑样条，**留一法交叉验证 (LOOCV)** 不需要训练 $n$ 次模型，只需训练 1 次即可算出精确结果！
$$RSS_{cv}(\lambda) = \sum_{i=1}^n \left( \frac{y_i - \hat{g}_\lambda(x_i)}{1 - \{ \mathbf{S}_\lambda \}_{ii}} \right)^2$$
* **实战意义**：在美赛中，这允许我们以极快的速度自动寻找最优 $\lambda$（即最优平滑度）。

## 关联笔记
* ⬅️ 基础对比：[[回归样条]]
* 🔗 理论相似：[[岭回归]] (同样的 L2 惩罚逻辑)