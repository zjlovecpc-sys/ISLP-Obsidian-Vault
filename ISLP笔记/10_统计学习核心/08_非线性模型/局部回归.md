---
tags:
  - 统计学习
  - 非线性模型
  - 局部方法
aliases:
  - Local Regression
  - LOESS
  - LOWESS
  - Span
  - 滑动窗口
---

# 局部回归 (Local Regression)

## 1. 核心算法：滑动窗口加权
局部回归（通常称为 **LOESS** 或 **LOWESS**）的逻辑是：预测 $x_0$ 处的 $y$ 时，只使用 $x_0$ 附近的训练点。

**算法步骤**：
1.  **选邻居**：收集距离 $x_0$ 最近的 $k = s \cdot n$ 个数据点。
    * 参数 **$s$ (Span/跨度)**：控制邻域大小，类似于 KNN 中的 $K$。
2.  **算权重**：为这些邻居分配权重 $K_{i0}$。距离 $x_0$ 越近的点权重越大，最远的邻居权重为 0。
3.  **做回归**：利用这些加权点，拟合一个**加权最小二乘回归 (Weighted Least Squares)**（通常是直线的或二次的）。
4.  **预测**：根据拟合出的局部模型计算 $\hat{f}(x_0)$。

## 2. 关键参数：跨度 $s$ (Span)
* **$s$ 较小**：邻域小，模型极其局部化，曲线剧烈波动。$\implies$ **低偏差，高方差**。
* **$s$ 较大**：邻域接近全集，模型接近全局最小二乘。$\implies$ **高偏差，低方差**。

## 3. 编程手视角
* **本质**：它是 [[K最近邻法]] 的回归增强版。KNN 取平均，LOESS 取局部回归。
* **实战用途**：非常适合用于**探索性数据分析 (EDA)** 中的散点图平滑，帮助你快速看清数据趋势（如 `seaborn.regplot(lowess=True)`）。
* **局限性**：在高维数据中表现极差（维数灾难），因为高维空间中“最近的邻居”也非常远。通常仅用于 $p=1$ 或 $p=2$。

## 关联笔记
* 🔗 算法亲戚：[[K最近邻法]]
* 🔗 拟合核心：[[最小二乘法#加权最小二乘 (WLS)]]