---
tags:
  - 统计学习
  - 无监督学习
  - 降维
  - 特征工程
  - 核心算法
aliases:
  - PCA
  - Principal Component Analysis
  - 主成分
  - Loading Vector
  - Score Vector
---

# 主成分分析 (Principal Component Analysis)

## 核心思想
**主成分分析 (PCA)** 是一种将一组可能相关的变量转换为一组线性不相关的变量（称为**主成分**）的数学变换方法。
* **几何直觉**：你可以把它想象成**坐标轴的旋转**。我们在高维空间中找到数据分布最“分散”（方差最大）的方向，把这个方向定义为新的 X 轴（第一主成分），然后找到与它垂直且分散度次之的方向作为 Y 轴（第二主成分），以此类推。



## 数学定义

### 1. 第一主成分 (First Principal Component)
第一主成分 $Z_1$ 是原始特征 $X_1, \dots, X_p$ 的线性组合，它捕获了数据中**最大**的变异（Variance）。

$$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + \dots + \phi_{p1}X_p$$

* **载荷向量 (Loading Vector)**：$\phi_1 = (\phi_{11}, \dots, \phi_{p1})^T$。它是我们在寻找的方向。
* **约束条件**：为了使结果唯一且方差不发散，必须限制载荷向量的模为 1，即 $\sum_{j=1}^p \phi_{j1}^2 = 1$。
* **优化目标**：寻找 $\phi_1$ 使得 $\text{Var}(Z_1)$ 最大化。

### 2. 后续主成分
第二主成分 $Z_2$ 是数据中变异第二大的线性组合，但必须满足一个强约束：**与 $Z_1$ 正交 (Uncorrelated)**。
这意味着 $Z_2$ 包含的信息是 $Z_1$ 完全没提到过的。

## 关键术语 (编程手必知)

| 术语 | 符号 | 解释 | 实战对应 (sklearn) |
| :--- | :--- | :--- | :--- |
| **载荷** (Loadings) | $\phi_{jm}$ | 每个原始变量在主成分中的权重。反映了变量的重要性。 | `pca.components_` |
| **得分** (Scores) | $z_{im}$ | 样本 $i$ 在第 $m$ 个主成分方向上的坐标值（变换后的数据）。 | `pca.transform(X)` |
| **PVE** | - | **方差解释比例** (Proportion of Variance Explained)。第 $m$ 个主成分解释了数据总方差的百分比。 | `pca.explained_variance_ratio_` |

## 如何选择主成分个数 ($M$)？
在降维时，我们通常只保留前 $M$ 个主成分。如何确定 $M$？

### 碎石图 (Scree Plot)
将每个主成分的 PVE 绘制成图。
* **选择标准**：寻找曲线的 **“肘部” (Elbow)** —— 即 PVE 边际增益迅速下降、曲线趋于平缓的点。这说明后续的主成分主要包含的是噪声而非信号。



## 美赛实战铁律

### 1. 必须标准化 (Standardization)
PCA 对变量的尺度极度敏感。
* 如果变量 $X_1$ 的单位是“米”，$X_2$ 的单位是“毫米”，$X_2$ 的方差会比 $X_1$ 大 100 万倍。PCA 会误以为 $X_2$ 包含了所有信息，导致第一主成分完全由 $X_2$ 主导。
* **操作**：在做 PCA 之前，**必须**对数据进行 [[数据标准化]]（均值为 0，方差为 1）。

### 2. 在回归中的应用 (PCR)
* 详情见 [[降维方法#1. 主成分回归 (Principal Components Regression, PCR)]]。
* 核心逻辑：既然 $Z_1, \dots, Z_M$ 集中了数据的大部分波动，那么用它们来预测 $Y$ 往往比用原始的 $X$ 更稳健（去噪）且计算更快。

## 关联笔记
* [[降维方法]]：PCA 在监督学习中的应用。
* [[数据标准化]]：PCA 的前置步骤。
* [[无监督学习]]：PCA 的归属类别。