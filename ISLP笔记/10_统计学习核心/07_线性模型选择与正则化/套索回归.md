---
tags:
  - 统计学习
  - 正则化
  - Lasso
  - 特征选择
aliases:
  - The Lasso
  - L1 Regularization
  - L1范数
---

# 套索回归 (The Lasso)

## 核心定义
Lasso (Least Absolute Shrinkage and Selection Operator) 使用 **L1 惩罚项** 代替了岭回归的 L2 惩罚项。其目标是最小化：

$$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p |\beta_j| = RSS + \lambda \|\beta\|_1$$

* **$\lambda \sum |\beta_j|$**：**L1 惩罚项**。

## 核心优势：稀疏性 (Sparsity)
这是 Lasso 与岭回归最大的区别。
* **特征选择**：当 $\lambda$ 足够大时，L1 惩罚项会迫使部分系数估计值**精确地等于零**。
* **结果**：Lasso 会产生一个**稀疏模型**（只包含变量子集），这使得模型极具**可解释性**。

## 几何解释 (L1 vs L2)
为什么 Lasso 能让系数为 0 而岭回归不能？
* **岭回归 (L2)**：约束区域是**圆形**（球体）。RSS 等高线与圆形相切的点，几乎不可能正好在坐标轴上。
* **Lasso (L1)**：约束区域是**菱形**（多面体），有尖角。RSS 等高线非常容易在**尖角**处（即坐标轴上）与约束区域相切，此时对应的系数为 0。

## 软阈值化 (Soft-Thresholding)
在正交设计矩阵的特殊情况下：
* **岭回归**：按比例收缩系数 ($\hat{\beta}_{LS} / (1+\lambda)$)。
* **Lasso**：进行**软阈值化**。
    * 如果 $|\hat{\beta}_{LS}| < \lambda/2$，则系数被置为 0。
    * 否则，系数向 0 的方向平移 $\lambda/2$。

## 实战选择：Lasso 还是 Ridge？
* 如果真实模型中**只有少数几个**变量起作用（稀疏），**Lasso** 表现更好。
* 如果真实模型中**很多变量**都起作用且系数都很小（稠密），**Ridge** 表现更好。
* **方案**：使用交叉验证比较两者的测试误差。