---
tags:
  - 统计学习
  - 正则化
  - 岭回归
  - 偏差-方差权衡
aliases:
  - Ridge Regression
  - L2 Regularization
  - L2范数
---

# 岭回归 (Ridge Regression)

## 核心定义
岭回归是在最小二乘法的 RSS 基础上，加入了一个**收缩惩罚项 (Shrinkage Penalty)**。其系数估计值 $\hat{\beta}^R$ 是通过最小化以下目标函数得到的：

$$\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \beta_j^2 = RSS + \lambda \|\beta\|_2^2$$

* **$\lambda \ge 0$**：**调节参数 (Tuning Parameter)**。控制惩罚的力度。
* **$\lambda \sum \beta_j^2$**：**L2 惩罚项**。它迫使系数尽可能小（收缩向零）。
* **注意**：截距项 $\beta_0$ 通常不接受惩罚。

## 关键性质
1.  **收缩效应**：随着 $\lambda$ 增大，系数估计值 $\hat{\beta}_\lambda^R$ 逐渐趋向于 0，但**永远不会等于 0**（除非 $\lambda = \infty$）。因此，岭回归**不具有**特征选择功能。
2.  **尺度敏感性**：标准最小二乘法是尺度不变的（$X$ 乘以常数，$Y$ 预测值不变）。但岭回归**对尺度敏感**。
    * **实战铁律**：在使用岭回归之前，**必须**先对预测变量进行**标准化 (Standardization)**，使其具有相同的尺度（如标准差为 1）。
3.  **有效自由度 (Effective Degrees of Freedom)**
    * 在最小二乘法中，自由度 $df = p$（变量个数）。
    * 在岭回归中，虽然所有 $p$ 个系数都不为 0，但由于收缩效应，模型的**有效自由度** $df(\lambda)$ 小于 $p$。
    * **公式**：$df(\lambda) = \text{tr}(\mathbf{H}_\lambda) = \sum_{j=1}^p \frac{d_j^2}{d_j^2 + \lambda}$
        * 其中 $d_j$ 是 $X$ 的奇异值。
        * $\lambda = 0$ 时，$df = p$；$\lambda \to \infty$ 时，$df \to 0$。
    * **意义**：这解释了为什么岭回归即使保留了所有变量，却依然能防止过拟合——因为它限制了模型在某些方向上的自由度。

## 为什么优于最小二乘法？
* **偏差-方差权衡**：
    * 当 $\lambda$ 增加时，模型的灵活性降低 $\rightarrow$ **方差 (Variance) 降低**，**偏差 (Bias) 增加**。
    * 在变量多、共线性强的情况下，最小二乘估计的方差极大。岭回归通过引入少量偏差，换取了方差的大幅降低，从而降低了整体的 MSE。
* **计算优势**：即使在 $p > n$ 的情况下，岭回归依然有唯一解，而最小二乘法无解。

## 关联笔记
* 对比：[[套索回归]]
* 预处理：[[数据标准化]] 