---
tags:
  - 统计学习
  - 降维
  - PCR
  - PLS
  - 无监督学习
aliases:
  - Dimension Reduction Methods
  - Principal Components Regression
  - Partial Least Squares
---

# 降维方法 (Dimension Reduction Methods)

## 核心思想
不同于**子集选择**（剔除变量）和**正则化**（收缩系数），**降维方法**通过转换原始预测变量 $X_1, \dots, X_p$ 来构建一组新的预测变量 $Z_1, \dots, Z_M$（其中 $M < p$），然后使用这 $M$ 个新变量拟合最小二乘模型。

数学形式为：
$$Z_m = \sum_{j=1}^p \phi_{jm} X_j$$
$$y_i = \theta_0 + \sum_{m=1}^M \theta_m z_{im} + \epsilon_i$$

这一过程将高维问题（$p$）转化为低维问题（$M$），从而缓解过拟合。

## 1. 主成分回归 (Principal Components Regression, PCR)
PCR 是 **[[主成分分析（待后期完善）]] (PCA)** 与 **最小二乘法** 的结合。

### 算法流程
1.  **构造主成分**：对预测变量矩阵 $X$ 进行 PCA，得到前 $M$ 个主成分 $Z_1, \dots, Z_M$。
    * $Z_1$ 是数据变异（方差）最大的方向。
    * $Z_2$ 是与 $Z_1$ 正交且剩余方差最大的方向，依此类推。
2.  **回归建模**：将 $Y$ 对这 $M$ 个主成分进行最小二乘回归。

### 核心假设与优势
* **假设**：PCR 假设预测变量中**变异最大**（方差最大）的方向，也正是与响应变量 $Y$ **最相关**的方向。
* **去噪**：通常后几个主成分（方差极小）主要包含噪声。丢弃它们可以提高信噪比。
* **无监督特性**：构造 $Z$ 的过程完全不看 $Y$（无监督）。这既是优点（防止过拟合），也是潜在弱点（可能丢掉与 $Y$ 相关但方差小的方向）。

### 实战注意
* **数据标准化**：在做 PCR 之前，**必须**对 $X$ 进行标准化（均值为 0，方差为 1），否则主成分会由方差大的变量主导。
* **参数选择**：$M$ 的选择通常通过 [[交叉验证实战|交叉验证]] 确定。

## 2. 偏最小二乘 (Partial Least Squares, PLS)
PLS 是 PCR 的**监督学习**版本。

### 与 PCR 的区别
* **PCR**：只利用 $X$ 的信息来寻找 $Z$（最大化 $\text{Var}(Z)$）。
* **PLS**：同时利用 $X$ 和 $Y$ 的信息来寻找 $Z$。它试图找到一个方向，既能解释 $X$ 的方差，又能解释 $Y$ 的变异（最大化 $\text{Corr}^2(Y, X) \cdot \text{Var}(X)$）。

### 算法细节
* 第一个 PLS 方向计算每个 $X_j$ 与 $Y$ 的简单回归系数。系数越大，说明该变量与 $Y$ 越相关，在构造 $Z_1$ 时的权重就越大。

### 美赛选型指南
* 虽然 PLS 理论上更合理（利用了 $Y$），但在实战中，**PLS 的表现通常并不比 PCR 好多少**。
* PCR 更加稳健，且不仅用于预测，常作为探索性数据分析（EDA）的工具。首选 PCR。

## 关联笔记
* [[主成分分析（待后期完善）]]：详细的 PCA 数学原理。
* [[正则化调参指南]]：如何确定 $M$。