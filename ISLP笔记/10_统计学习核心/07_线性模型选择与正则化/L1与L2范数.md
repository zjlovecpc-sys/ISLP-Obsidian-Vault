---
tags:
  - 统计学习
  - 数学基础
  - 正则化
  - 范数
aliases:
  - L1 Norm
  - L2 Norm
  - 曼哈顿距离
  - 欧几里得距离
  - Weight Decay
---

# L1与L2范数 (L1 & L2 Norm)

## 核心定义

在统计学习中，**范数 (Norm)** 是衡量向量“长度”或“大小”的数学工具。对于一个 $p$ 维向量 $\beta = (\beta_1, \beta_2, \dots, \beta_p)$，最常用的两种范数如下：

### 1. L1 范数 (L1 Norm)
各元素绝对值之和。对应几何中的**曼哈顿距离 (Manhattan Distance)**。
$$||\beta||_1 = \sum_{j=1}^p |\beta_j| = |\beta_1| + |\beta_2| + \dots + |\beta_p|$$

* **形状**：在二维平面上，单位球（$||\beta||_1 = 1$）呈**菱形**（Diamond shape），有尖锐的角点。
* **核心应用**：[[套索回归]] (Lasso)。

### 2. L2 范数 (L2 Norm)
各元素平方和的平方根。对应几何中的**欧几里得距离 (Euclidean Distance)**。
$$||\beta||_2 = \sqrt{\sum_{j=1}^p \beta_j^2} = \sqrt{\beta_1^2 + \beta_2^2 + \dots + \beta_p^2}$$
*(注：在正则化公式中，我们通常直接使用平方形式 $||\beta||_2^2 = \sum \beta_j^2$ 以消除根号方便求导)*

* **形状**：在二维平面上，单位球（$||\beta||_2 = 1$）呈**圆形** (Circle)，边界光滑。
* **核心应用**：[[岭回归]] (Ridge)。

## 几何性质与稀疏性 (Sparsity)

为什么 L1 能做特征选择而 L2 不能？这取决于它们的**等高线形状**。

### 优化视角
正则化问题的本质是在**约束条件**（范数 $\le s$）下最小化 **RSS**（椭圆等高线）。最优解出现在 RSS 等高线与范数约束区域的**切点**上。

1.  **L1 (菱形)**：
    * 菱形有突出的**角点 (Corners)**，这些角点位于坐标轴上（即某些 $\beta_j = 0$）。
    * RSS 的椭圆往往最先接触到这些角点。
    * **结果**：产生**稀疏解**（Sparse Solution），即部分系数精确为 0。

2.  **L2 (圆形)**：
    * 圆形各个方向都是凸出的，没有角点。
    * RSS 的椭圆几乎总是与圆周的非坐标轴位置相切。
    * **结果**：产生**稠密解**（Dense Solution），系数会变小但不会等于 0。

## 编程手实战指南

在 Python (`sklearn`) 和深度学习中，这两个术语直接对应不同的参数设置：

| 概念 | 数学符号 | sklearn 参数 (`linear_model`) | 深度学习术语 |
| :--- | :--- | :--- | :--- |
| **L1 范数** | $\lambda \sum \|\beta_j\|$ | `penalty='l1'` | L1 Regularization |
| **L2 范数** | $\lambda \sum \beta_j^2$ | `penalty='l2'` | **Weight Decay** (权重衰减) |
| **混合 (Elastic Net)** | $\lambda_1 L_1 + \lambda_2 L_2$ | `penalty='elasticnet'` | Elastic Net |

## 关联笔记

* **应用场景 1**：[[套索回归]] (基于 L1)
* **应用场景 2**：[[岭回归]] (基于 L2)
* **前置操作**：由于范数计算依赖数值大小，计算前必须进行 [[数据标准化]]。