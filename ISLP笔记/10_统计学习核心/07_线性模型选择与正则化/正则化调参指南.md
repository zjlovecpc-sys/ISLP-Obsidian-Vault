---
tags:
  - 统计学习
  - 模型选择
  - 调参
  - 交叉验证
  - 备赛资料
aliases:
  - Tuning Parameter Selection
  - One-Standard-Error Rule
  - 一倍标准误准则
---

# 正则化调参指南 (Regularization Tuning Guide)

## 核心任务
在第六章的所有方法中，我们都面临一个**超参数 (Hyperparameter)** 选择问题：
* **岭回归/Lasso**：调节参数 $\lambda$。
* **子集选择/PCR/PLS**：模型大小/主成分个数 $M$ (或 $k$)。

这些参数无法通过最小二乘法直接估计，必须通过**重采样方法**来确定。

## 标准调参流程 (The Grid Search & CV)
1.  **构建网格**：设定一系列候选值。例如 $\lambda \in \{0.001, 0.01, \dots, 100\}$。
2.  **交叉验证**：对每一个候选值，进行 K-Fold CV（通常 $K=5$ 或 $10$）。
3.  **计算误差**：记录每个候选值对应的平均 CV 误差（Mean CV Error）及其标准误（Standard Error）。
4.  **选择参数**：
    * **常规法**：直接选择 CV 误差最小的那个值。
    * **一倍标准误法则**：见下文。
5.  **最终拟合**：选定最优参数后，使用**所有训练数据**重新拟合一次模型，得到最终的系数。

## 一倍标准误法则 (One-Standard-Error Rule)
这是美赛中体现“专业性”的高级技巧。
* **现象**：CV 误差曲线通常底部平缓。最小值附近的一系列模型性能差异微乎其微。
* **法则**：不要直接选 CV 误差最小的模型。而是在**CV 误差最小值的 1 倍标准误范围内**，选择**最简单**（$\lambda$ 最大或变量最少）的模型。
* **理由**：**奥卡姆剃刀原则**。如果两个模型性能差不多，永远选择更简单的那个，因为它泛化能力更强，过拟合风险更小。

## 高维数据处理 ($p > n$)
当预测变量数 $p$ 超过样本量 $n$ 时：
1.  **线性回归失效**：最小二乘法有无穷多解，RSS 恒为 0，测试误差极大。
2.  **必须使用