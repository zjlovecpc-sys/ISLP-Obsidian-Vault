---
tags:
  - 统计学习
  - 特征选择
  - 核心算法
aliases:
  - Subset Selection
  - Best Subset Selection
  - Forward Stepwise Selection
  - Backward Stepwise Selection
---

# 子集选择法 (Subset Selection)

## 核心目标
在回归分析中，我们可能有 $p$ 个预测变量。我们希望从这 $p$ 个变量中筛选出一个子集，使得基于该子集训练的模型不仅在训练集上表现良好，更能准确预测未来数据（低测试误差）。

## 1. 最优子集选择 (Best Subset Selection)
这是一种穷举式的搜索方法。

### 算法流程
1.  记 $\mathcal{M}_0$ 为**空模型**（不含任何预测变量，只预测样本均值）。
2.  对于 $k = 1, 2, \dots, p$：
    * 拟合所有 $\binom{p}{k}$ 个包含 $k$ 个预测变量的模型。
    * 在这些模型中，挑选出 **RSS 最小**（或 $R^2$ 最大）的那个，记为 $\mathcal{M}_k$。
3.  最终，在 $\mathcal{M}_0, \mathcal{M}_1, \dots, \mathcal{M}_p$ 这 $p+1$ 个候选模型中，使用 [[模型选择指标]]（如 $C_p$, BIC, AIC, 调整 $R^2$）或 [[交叉验证实战|交叉验证]] 选出唯一的最佳模型。

### 优缺点
* **优点**：简单直接，保证能找到训练数据上 RSS 最小的组合。
* **缺点**：计算量巨大。需拟合 $2^p$ 个模型。当 $p=20$ 时，模型数量超过 100 万，计算不可行。
* **风险**：搜索空间过大，极易导致**过拟合**。

## 2. 前向逐步选择 (Forward Stepwise Selection)
为了解决计算量问题，采用贪心算法策略。

### 算法流程
1.  记 $\mathcal{M}_0$ 为**空模型**。
2.  对于 $k = 0, \dots, p-1$：
    * 在当前模型 $\mathcal{M}_k$ 的基础上，**尝试添加** 剩余 $p-k$ 个变量中的每一个。
    * 选择能使 **RSS 降幅最大**（或 $R^2$ 增幅最大）的那个变量加入模型，形成 $\mathcal{M}_{k+1}$。
3.  最终，在 $\mathcal{M}_0, \dots, \mathcal{M}_p$ 中，使用交叉验证或评估指标选出最佳模型。

### 优缺点
* **计算优势**：只需拟合 $1 + p(p+1)/2$ 个模型。当 $p=20$ 时，仅需拟合 211 个模型。
* **局限性**：无法保证找到全局最优解。例如，某个变量在早期被引入，但随着后期其他变量的加入，它可能变得不重要，但在前向过程中无法将其剔除。

## 3. 后向逐步选择 (Backward Stepwise Selection)
与前向相反，先由全模型开始，逐个剔除最不重要的变量。

### 算法流程
1.  记 $\mathcal{M}_p$ 为**全模型**（包含所有 $p$ 个变量）。
2.  对于 $k = p, p-1, \dots, 1$：
    * 在当前模型 $\mathcal{M}_k$ 中，**尝试剔除** 每一个变量。
    * 选择剔除后对模型拟合效果**损失最小**（RSS 增加最少）的那个变量，形成 $\mathcal{M}_{k-1}$。
3.  最终，在 $\mathcal{M}_0, \dots, \mathcal{M}_p$ 中选出最佳模型。

### 适用条件
* **限制**：必须满足 $n > p$（样本量大于变量数），否则无法拟合初始的全模型 $\mathcal{M}_p$。而前向选择法在 $n < p$ 时依然可用（最多选到 $\mathcal{M}_{n-1}$）。

## 关联笔记
* [[模型选择指标]]：用于步骤 3 的最终决策。
* [[交叉验证实战]]：现代建模中更常用的选择标准。