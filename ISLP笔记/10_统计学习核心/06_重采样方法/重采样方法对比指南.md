---
tags:
  - 统计学习
  - 重采样
  - 模型选择
  - 备赛资料
aliases:
  - Resampling Comparison
  - CV vs Bootstrap
  - 交叉验证选择指南
---

# 重采样方法对比与选择指南

## 核心对比表 (Cheat Sheet)

在美赛编程中，选择哪种方法主要取决于**数据量大小**、**计算资源**以及**任务目标**。

| 维度 | [[验证集方法]] | [[留一法交叉验证]] (LOOCV) | [[K折交叉验证]] (K=5/10) | [[自助法]] (Bootstrap) |
| :--- | :--- | :--- | :--- | :--- |
| **计算成本** | **极低** (跑 1 次) | **极高** (跑 $n$ 次)<br>*(线性回归除外)* | **中等** (跑 $K$ 次) | **高** (跑 $B \approx 1000$ 次) |
| **偏差 (Bias)** | **高** (数据利用率低) | **极低** (几乎用了所有数据) | **低** (介于两者之间) | 低 |
| **方差 (Variance)** | **高** (结果依赖随机切分) | **高** (模型高度相关) | **低** (最佳平衡点) | - |
| **随机性** | 有 (切分不同结果不同) | **无** (结果唯一) | 有 (但比验证集方法稳定) | 有 (依赖随机抽样) |
| **主要用途** | 快速评估超大数据集 | 小样本模型评估 | **通用模型评估与选择** | **参数标准误/置信区间估计** |

## 场景选择指南 (Scenario Guide)

### 场景 1：通用的模型选择 (Default)
* **推荐**：**[[K折交叉验证]] (K=5 或 10)**。
* **理由**：这是偏差和方差权衡的“甜蜜点” (Sweet Spot)。既不会像验证集那样浪费数据，也不会像 LOOCV 那样计算缓慢且方差大。它是学术界和工业界的**黄金标准**。

### 场景 2：数据量极小 ($n < 50$)
* **推荐**：**[[留一法交叉验证]] (LOOCV)**。
* **理由**：数据太宝贵了，不能浪费任何一点在验证集上。LOOCV 每次用 $n-1$ 个样本训练，能最大程度地利用信息。虽然方差高一点，但此时“低偏差”更重要。

### 场景 3：数据量巨大 ($n > 100,000$)
* **推荐**：**[[验证集方法]]**。
* **理由**：
    1.  当 $n$ 很大时，即便只用 50% 训练，模型也能学得很好（偏差不再是问题）。
    2.  计算成本是首要考虑。跑 10 次 K-Fold 可能太慢了，跑 1 次验证集就足够了。
### 场景 4：线性回归 (Linear Regression)
* **推荐**：**[[留一法交叉验证]] (LOOCV)**。
* **理由**：利用 [[留一法交叉验证#💡 计算捷径与解析性质 (Computational Shortcut)|魔法公式]] $\frac{1}{1-h_i}$，计算成本和跑一次模型一样。既然免费，为什么不选偏差最低的 LOOCV 呢？
* **注意**：此“魔法公式”仅适用于标准的 **OLS 线性回归**。如果你使用了 Lasso、Ridge 或 Elastic Net，该公式失效，必须老老实实跑计算。

### 场景 5：估计复杂统计量的置信区间
* **推荐**：**[[自助法]] (Bootstrap)**。
* **理由**：比如你要预测“未来收益的中位数”的波动范围，或者“两个模型预测结果之比”的标准误。交叉验证做不到这一点，只有 Bootstrap 能通过模拟采样给出答案。

## 深度辨析：为什么 Bootstrap 不常用于模型评估？

虽然 Bootstrap 也可以用来估计预测误差（如 .632 Bootstrap 方法），但在 ISLP 的框架下，我们通常**不推荐**用它来评估模型性能（如计算测试 MSE）。

* **原因**：Bootstrap 数据集包含重复样本。如果模型过拟合（死记硬背），它在“训练集”里见过的重复样本上表现会非常好，导致我们**严重低估**了错误率。
* **结论**：术业有专攻。**CV 负责评估预测准不准，Bootstrap 负责评估参数算得稳不稳。**