---
tags:
  - 统计学习
  - 重采样
  - LOOCV
  - 核心算法
aliases:
  - Leave-One-Out Cross-Validation
  - LOOCV
---

# 留一法交叉验证 (LOOCV)

## 核心原理
**LOOCV** 是验证集方法的极端特例。如果总共有 $n$ 个样本，我们进行 $n$ 次分割：
* **测试集**：只包含 **1** 个样本 $(x_i, y_i)$。
* **训练集**：包含剩余的 **$n-1$** 个样本。

我们重复这个过程 $n$ 次，每次留下不同的样本作为测试集。

## 计算公式
LOOCV 的估计误差是这 $n$ 次测试误差的平均值：

$$CV_{(n)} = \frac{1}{n} \sum_{i=1}^n MSE_i = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

## 优缺点分析

### 优点
1.  **无随机性**：不像[[验证集方法]]那样依赖随机分割。针对同一份数据，LOOCV 的结果是**唯一**的。
2.  **低偏差 (Low Bias)**：每次都用了 $n-1$ 个样本训练（几乎是全部数据），所以它非常接近用全部数据训练的真实表现。

### 缺点
1.  **计算代价高**：如果你有 100万条数据，模型就要跑 100万次。对于复杂模型（如逻辑回归、神经网络）是不可接受的。
2.  **方差较高**：这是一个反直觉的结论，详见 [[K折交叉验证#偏差-方差权衡 (LOOCV vs K-Fold)]]。

##  计算捷径与解析性质 (Computational Shortcut)
在 **[[最小二乘法]]** 的 **[[简单线性回归|线性回归]]** 或多项式回归中，LOOCV 拥有一个极其重要的数学性质：**我们不需要真的跑 $n$ 次模型！**

只需要跑 **1** 次模型，就可以算出 LOOCV 误差：

$$CV_{(n)} = \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2$$

* $\hat{y}_i$：原始模型（用所有数据训练）的预测值。
 - $h_i$：第 $i$ 个样本的 **[[回归诊断与潜在问题#5. 高杠杆点 (High Leverage Points)|杠杆值 (Leverage)]]**。
* **原理**：$h_i$ 反映了第 $i$ 个点对拟合线的影响力。如果 $h_i$ 很大（高杠杆），去掉它会对模型影响很大，所以公式里分母变小（$1-h_i$），使得该点的残差被放大（惩罚）。

**适用范围**：这个公式仅适用于最小二乘回归。对于非线性模型（如 KNN），仍需老老实实计算 $n$ 次。

实战链接：[[交叉验证实战#2. 留一法交叉验证 (LOOCV)]]