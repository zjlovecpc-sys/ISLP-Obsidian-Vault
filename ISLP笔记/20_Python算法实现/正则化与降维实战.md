---
tags:
  - Pythonå®æˆ˜
  - çº¿æ€§æ¨¡å‹
  - æ­£åˆ™åŒ–
  - é™ç»´
  - Sklearn
aliases:
  - Regularization Lab
  - Lasso Code
  - Ridge Code
  - PCR Code
---

# æ­£åˆ™åŒ–ä¸é™ç»´å®æˆ˜ (Regularization & Dimension Reduction)

## 0. ç¼–ç¨‹æ‰‹æ ¸å¿ƒç›´è§‰

åœ¨å¤„ç† $p > n$ æˆ–å¤šé‡å…±çº¿æ€§é—®é¢˜æ—¶ï¼Œå•çº¯çš„ `LinearRegression` ä¼šå¤±æ•ˆã€‚æœ¬æ¨¡å—æä¾›ä¸‰å¥—è§£å†³æ–¹æ¡ˆçš„ä»£ç æ¨¡æ¿ï¼š

1. **å²­å›å½’ (Ridge)**ï¼šä»…å‹ç¼©ç³»æ•°ï¼Œä¿ç•™æ‰€æœ‰å˜é‡ã€‚é€‚ç”¨äºâ€œè™½å¤šä½†éƒ½æœ‰ç”¨â€çš„åœºæ™¯ã€‚
2. **å¥—ç´¢å›å½’ (Lasso)**ï¼šå‹ç¼©ç³»æ•°å¹¶ç½®é›¶ã€‚é€‚ç”¨äºâ€œå˜é‡å¤ªå¤šï¼Œéœ€è¦æ¸…æ´—â€çš„åœºæ™¯ã€‚
3. **PCR/PLS**ï¼šå°†å˜é‡å‹ç¼©ä¸ºå‡ ä¸ªä¸»æˆåˆ†/æ–¹å‘ã€‚é€‚ç”¨äºâ€œå˜é‡é«˜åº¦ç›¸å…³â€çš„åœºæ™¯ã€‚

**âš ï¸ å·¥ç¨‹é“å¾‹ (The Pipeline Rule)**ï¼š
* æ‰€æœ‰æ­£åˆ™åŒ–å’Œé™ç»´æ–¹æ³•å¯¹**æ•°æ®çš„å°ºåº¦ (Scale)** æåº¦æ•æ„Ÿã€‚
* **å¿…é¡»**å…ˆè¿›è¡Œ [[æ•°æ®æ ‡å‡†åŒ–]] (Standardization)ã€‚
* **ä¸¥ç¦**åœ¨äº¤å‰éªŒè¯ (CV) å¾ªç¯å¤–åšå…¨å±€æ ‡å‡†åŒ–ï¼Œå¿…é¡»ä½¿ç”¨ `Pipeline` å°†æ ‡å‡†åŒ–æ­¥éª¤å°è£…åœ¨ CV å†…éƒ¨ï¼Œé˜²æ­¢**æ•°æ®æ³„éœ²**ã€‚

---

## 1. å²­å›å½’ (Ridge Regression)

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[å²­å›å½’]] | [[L1ä¸L2èŒƒæ•°#2. L2 èŒƒæ•° (L2 Norm)]]

### 1.1 æ ¸å¿ƒé€»è¾‘ä¸é™·é˜±
* **API å·®å¼‚**ï¼šISLP ç†è®ºç”¨ $\lambda$ï¼Œ`sklearn` ä»£ç ç”¨ `alpha`ã€‚
* **å®ç°æ–¹å¼**ï¼š
    * `Ridge()`: æ ‡å‡†å®ç°ï¼Œä½¿ç”¨ Cholesky åˆ†è§£ï¼Œé€Ÿåº¦å¿«ã€‚
    * `ElasticNet(l1_ratio=0)`: è¿­ä»£æ³•ï¼Œé€Ÿåº¦æ…¢ï¼Œä½†æ–¹ä¾¿ç”»â€œè·¯å¾„å›¾â€ã€‚

### 1.2 å²­è¿¹å›¾ç»˜åˆ¶ (Coefficient Path)
ç”¨äºè§‚å¯Ÿéšç€æƒ©ç½šåŠ›åº¦ $\lambda$ å¢å¤§ï¼Œç³»æ•°å¦‚ä½•è¢«å‹ç¼©ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

# 1. å‡†å¤‡æ•°æ® (å¿…é¡»æ ‡å‡†åŒ–ï¼Œè¿™é‡Œä¸ºäº†ç”»å›¾æ¼”ç¤ºæ‰‹åŠ¨å¤„ç†ï¼Œå®æˆ˜ç”¨Pipeline)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. ç”Ÿæˆ lambda åºåˆ— (å¯¹æ•°åˆ»åº¦)
# æ³¨æ„ï¼šRidge çš„ alpha å¯¹åº” ISLP çš„ lambda
alphas = 10**np.linspace(8, -2, 100) 

# 3. å¾ªç¯è®¡ç®—ç³»æ•°
coefs = []
for a in alphas:
    ridge = Ridge(alpha=a)
    ridge.fit(X_scaled, y)
    coefs.append(ridge.coef_)

# 4. ç»˜å›¾
plt.figure(figsize=(10, 6))
ax = plt.gca()
ax.plot(alphas, coefs)
ax.set_xscale('log') # Xè½´ç”¨å¯¹æ•°åæ ‡
plt.xlabel('alpha (log scale)')
plt.ylabel('Standardized Coefficients')
plt.title('Ridge Trace: Shrinkage of Coefficients')
plt.axis('tight')
plt.show()
```

### 1.3 è‡ªåŠ¨åŒ–è°ƒå‚ (RidgeCV)

ç¾èµ›å®æˆ˜æ¨èä½¿ç”¨ `RidgeCV`ï¼Œå®ƒå†…ç½®äº†é«˜æ•ˆçš„ Leave-One-Out Cross-Validation (LOOCV)ã€‚

```python
from sklearn.linear_model import RidgeCV
from sklearn.pipeline import Pipeline

# æ„å»ºé˜²æ³„éœ²ç®¡é“
pipe_ridge = Pipeline([
    ('scaler', StandardScaler()),       # æ­¥éª¤1: è‡ªåŠ¨åŒ–æ ‡å‡†åŒ–
    ('ridge', RidgeCV(
        alphas=np.logspace(-2, 2, 100), # æœç´¢èŒƒå›´
        scoring='neg_mean_squared_error',
        cv=5                            # 5æŠ˜äº¤å‰éªŒè¯
    ))                                  # æ­¥éª¤2: å¸¦CVçš„å²­å›å½’
])

# è®­ç»ƒ
pipe_ridge.fit(X, y)

# è·å–æœ€ä½³æ¨¡å‹å‚æ•°
best_model = pipe_ridge.named_steps['ridge']
print(f"æœ€ä½³ alpha: {best_model.alpha_}")
print(f"æœ€ä½³ç³»æ•°: {best_model.coef_}")
```

---

## 2. å¥—ç´¢å›å½’ (The Lasso)

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[å¥—ç´¢å›å½’]] | [[L1ä¸L2èŒƒæ•°#1. L1 èŒƒæ•° (L1 Norm)]]

### 2.1 æ ¸å¿ƒé€»è¾‘

- **ç¨€ç–æ€§ (Sparsity)**ï¼šLasso ä¼šå°†ä¸é‡è¦çš„å˜é‡ç³»æ•°å¼ºè¡Œå‹ä¸º 0ã€‚
- **è¿­ä»£é—®é¢˜**ï¼šè‹¥å‡ºç° `ConvergenceWarning`ï¼Œéœ€å¢åŠ  `max_iter` (å¦‚ 10000)ã€‚

### 2.2 äº¤å‰éªŒè¯ä¸ç¨€ç–è§£å¯è§†åŒ–

æ­¤æ¨¡æ¿å±•ç¤º Lasso å¦‚ä½•éš $\alpha$ å˜åŒ–â€œå‰”é™¤â€å˜é‡ã€‚

```python
from sklearn.linear_model import LassoCV

# æ„å»ºç®¡é“
pipe_lasso = Pipeline([
    ('scaler', StandardScaler()),
    ('lasso', LassoCV(
        n_alphas=100,       # è‡ªåŠ¨ç”Ÿæˆ100ä¸ªalpha
        cv=5,               # 5æŠ˜äº¤å‰éªŒè¯
        max_iter=10000,     # é˜²æ­¢ä¸æ”¶æ•›è­¦å‘Š
        random_state=42
    ))
])

# è®­ç»ƒ
pipe_lasso.fit(X, y)
lasso_model = pipe_lasso.named_steps['lasso']

# ç»“æœåˆ†æ
print(f"æœ€ä½³ alpha: {lasso_model.alpha_}")
print(f"ä¿ç•™å˜é‡æ•°: {np.sum(lasso_model.coef_ != 0)} / {len(lasso_model.coef_)}")

# ç»˜åˆ¶ MSE æ›²çº¿ (Uå‹æ›²çº¿)
# lasso_model.mse_path_ å½¢çŠ¶ä¸º (n_alphas, n_folds)
mse_mean = lasso_model.mse_path_.mean(axis=1)
mse_std = lasso_model.mse_path_.std(axis=1) / np.sqrt(5)

plt.figure(figsize=(8, 6))
plt.errorbar(lasso_model.alphas_, mse_mean, yerr=mse_std, fmt='o-', color='r', label='CV MSE')
plt.axvline(lasso_model.alpha_, linestyle='--', color='k', label='Optimal Alpha')
plt.semilogx() # å¯¹æ•°Xè½´
plt.xlabel('alpha')
plt.ylabel('MSE')
plt.legend()
plt.show()
```

---

## 3. ä¸»æˆåˆ†å›å½’ (PCR)

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[é™ç»´æ–¹æ³•]] | [[ä¸»æˆåˆ†åˆ†æï¼ˆå¾…åæœŸå®Œå–„ï¼‰]]

### 3.1 æ ¸å¿ƒé€»è¾‘

- **æ— ç›‘ç£ + ç›‘ç£**ï¼šå…ˆç”¨ PCA (æ— ç›‘ç£) æå– $M$ ä¸ªä¸»æˆåˆ†ï¼Œå†ç”¨ OLS (ç›‘ç£) è¿›è¡Œå›å½’ã€‚
- **é™ç»´æ‰“å‡»**ï¼šè§£å†³å˜é‡é—´é«˜åº¦ç›¸å…³ï¼ˆå¤šé‡å…±çº¿æ€§ï¼‰çš„ç»ˆææ‰‹æ®µã€‚

### 3.2 å®Œæ•´æµæ°´çº¿å®ç°

æ­¤å¤„å¿…é¡»ä½¿ç”¨ GridSearch æ¥ç¡®å®šä¿ç•™å¤šå°‘ä¸ªä¸»æˆåˆ† ($M$)ã€‚

```python
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV

# 1. å®šä¹‰ä¸‰æ®µå¼ç®¡é“
pcr_pipe = Pipeline([
    ('scaler', StandardScaler()),      # å¿…é¡»æ ‡å‡†åŒ–ï¼Œå¦åˆ™PCAä¼šè¢«å¤§æ–¹å·®å˜é‡ä¸»å¯¼
    ('pca', PCA()),                    # é™ç»´
    ('linear', LinearRegression())     # å›å½’
])

# 2. å®šä¹‰å‚æ•°ç½‘æ ¼ (æœç´¢ä¿ç•™å¤šå°‘ä¸ªä¸»æˆåˆ†)
# æ³¨æ„ param_grid çš„å‘½åè§„åˆ™: "æ­¥éª¤å__å‚æ•°å"
param_grid = {
    'pca__n_components': range(1, X.shape[1] + 1)
}

# 3. ç½‘æ ¼æœç´¢ (Grid Search)
grid = GridSearchCV(
    pcr_pipe,
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error'
)

grid.fit(X, y)

# 4. è¾“å‡ºç»“æœ
print(f"æœ€ä½³ä¸»æˆåˆ†æ•° (M): {grid.best_params_['pca__n_components']}")
print(f"æœ€ä½ CV MSE: {-grid.best_score_}")

# 5. éªŒè¯æ¨¡å‹è§£é‡Šæ–¹å·®æ¯”ä¾‹ (Explained Variance)
best_pca = grid.best_estimator_.named_steps['pca']
print(f"ç´¯ç§¯è§£é‡Šæ–¹å·®: {np.cumsum(best_pca.explained_variance_ratio_)}")
```

---

## 4. åæœ€å°äºŒä¹˜æ³• (PLS)

**ğŸ”— ç†è®ºå›æº¯**ï¼š[[é™ç»´æ–¹æ³•#åæœ€å°äºŒä¹˜ (PLS)]]

ä¸ PCR çš„åŒºåˆ«ï¼šPCR æ˜¯æ— ç›‘ç£é™ç»´ï¼ˆåªçœ‹ X çš„æ–¹å·®ï¼‰ï¼ŒPLS æ˜¯ç›‘ç£é™ç»´ï¼ˆå¯»æ‰¾ä¸ Y ç›¸å…³æ€§æœ€å¤§çš„æ–¹å‘ï¼‰ã€‚

```python
from sklearn.cross_decomposition import PLSRegression

# PLSRegression è‡ªå¸¦ scale=Trueï¼Œä½†æ”¾å…¥ Pipeline æ˜¾å¼æ ‡å‡†åŒ–æ˜¯æ›´å¥½çš„ä¹ æƒ¯
pls_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('pls', PLSRegression())
])

# åŒæ ·ä½¿ç”¨ GridSearch å¯»æ‰¾æœ€ä½³ç»„ä»¶æ•°
param_grid_pls = {'pls__n_components': range(1, X.shape[1] + 1)}

grid_pls = GridSearchCV(pls_pipe, param_grid_pls, cv=5, scoring='neg_mean_squared_error')
grid_pls.fit(X, y)

print(f"PLS æœ€ä½³ç»„ä»¶æ•°: {grid_pls.best_params_['pls__n_components']}")
```

---

## 5. å¸¸è§é”™è¯¯é€ŸæŸ¥

| **ç°è±¡**                           | **åŸå› è¯Šæ–­**                  | **è§£å†³æ–¹æ¡ˆ**                           |
| :------------------------------- | :------------------------ | :--------------------------------- |
| **Ridge/Lasso æ•ˆæœæå·®**             | ææœ‰å¯èƒ½æ˜¯**æœªæ ‡å‡†åŒ–**æ•°æ®ã€‚          | æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† `StandardScaler`ã€‚          |
| **CV åˆ†æ•°è¿‡é«˜ (MSE å°å¾—ä¸æ­£å¸¸)**          | **æ•°æ®æ³„éœ²**ã€‚å…ˆåœ¨å…¨é›†ä¸Šæ ‡å‡†åŒ–ï¼Œå†æ‹†åˆ† CVã€‚ | æ”¹ç”¨ `Pipeline`ï¼ŒæŠŠ Scaler æ”¾è¿›å»ã€‚        |
| **Lasso æŠ¥ `ConvergenceWarning`** | è¿­ä»£æ¬¡æ•°ä¸å¤Ÿæˆ–æ•°æ®æœªæ ‡å‡†åŒ–ã€‚            | å¢åŠ  `max_iter=10000` æˆ–æ£€æŸ¥ Scalerã€‚    |
| **`ElasticNet` è­¦å‘Š**              | `l1_ratio=0` æ—¶è¯•å›¾æ¨¡æ‹Ÿ Ridgeã€‚ | ä½¿ç”¨ `Ridge` ç±»ä»£æ›¿ `ElasticNet`ï¼Œæˆ–å¿½ç•¥è­¦å‘Šã€‚ |