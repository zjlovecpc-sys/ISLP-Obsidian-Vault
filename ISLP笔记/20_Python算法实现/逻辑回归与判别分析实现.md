---
tags:
  - Python
  - 逻辑回归
  - LDA
  - QDA
  - sklearn
aliases:
  - Classification Lab
  - confusion_table
---

# 逻辑回归与判别分析实战

## 1. 数据探索与可视化

在处理时间序列数据（如股票 Smarket）时，Pandas 的绘图功能非常强大。

```python
# 技巧：显式创建画布，控制分辨率
fig, ax = plt.subplots(figsize=(10, 6), dpi=100)
Smarket.plot(y='Volume', kind='line', ax=ax, color='blue', alpha=0.6)
ax.grid(True, linestyle='--')
```

- **避坑**：`Smarket.corr()` 可能会报错，因为包含字符串列。
- **正解**：`Smarket.corr(numeric_only=True)` 或 `Smarket.drop(columns=['Direction']).corr()`。

## 2. 逻辑回归 (Statsmodels)

为了获得详细的统计报表（P值、系数显著性），我们首选 `statsmodels`。

```python
import statsmodels.api as sm

# family=sm.families.Binomial() 是核心！
# 它告诉 GLM：“请使用 Logit 链接函数，因为我是二分类问题。”
# 此处的 X_train 必须是已经包含截距列的设计矩阵。
glm = sm.GLM(y_train, X_train, family=sm.families.Binomial())
results = glm.fit()
print(summarize(results))
```

- **预测**：`probs = results.predict(exog=X_test)` 输出的是概率。
- **阈值化**：必须手动将概率转换为类别（通常以 0.5 为界）。

## 3. 判别分析 (Sklearn)

对于 LDA/QDA，我们更关注预测性能，因此切换到 `sklearn`。

### Linear Discriminant Analysis (LDA)

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# store_covariance=True 是为了后续能查看协方差矩阵 (ISLP 特色)
lda = LDA(store_covariance=True)
# 注意：Sklearn 自动处理截距，输入 X 不要包含 intercept 列！
lda.fit(X_train, y_train)
```

- **核心属性**：
  - `lda.means_`: 各类别的均值向量（用户画像）。
  - `lda.scalings_`: 线性判别系数。

- **模型对比 (LDA vs 逻辑回归)**：
  > _来自你的实验总结：_
  > 
  > 1. **类别分得开** $\to$ **LDA** (逻辑回归不稳定)。
  > 2. **小样本** $\to$ **LDA** (方差更小，防过拟合)。
  > 3. **多分类** $\to$ **LDA** (天然支持)。
  > 4. **解释性** $\to$ **逻辑回归** (系数含义清晰)。
  
### 关键参数详解
- **`store_covariance=True`**：
    - **默认行为**：Sklearn 为了节省内存，默认**不保存**协方差矩阵。它认为你只需要预测结果 (`predict`)，不需要看中间参数。
    - **为什么改为 True？**：在 ISLP 实验或美赛分析中，我们经常需要**打印协方差矩阵** (`lda.covariance_`) 来分析特征之间的相关性，或者验证 LDA 的“同方差”假设是否成立。

### 属性对照表 (Sklearn)
不同模型的属性名称略有不同，但数学含义一致：

| 统计学含义              | LDA 属性        | QDA 属性             | 朴素贝叶斯 (GaussianNB) 属性 |
| :----------------- | :------------ | :----------------- | :-------------------- |
| **类均值 ($\mu_k$)**  | `means_`      | `means_`           | `theta_` (注意改名了！)     |
| **先验概率 ($\pi_k$)** | `priors_`     | `priors_`          | `class_prior_`        |
| **协方差 ($\Sigma$)** | `covariance_` | `covariance_` (列表) | `var_` (仅存方差，因为假设独立)  |
| **判别系数**           | `scalings_`   | (无，边界非线性)          | (无)                   |

### Quadratic Discriminant Analysis (QDA)

```python
qda = QDA(store_covariance=True)
qda.fit(X_train, y_train)
```

- **特点**：假设各类别的协方差矩阵不同，决策边界是弯曲的。

## 4. 朴素贝叶斯 (Naive Bayes)

```python
from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)
```

- **特点**：假设特征之间相互独立。虽然假设很强，但在高维数据下表现往往不错。
- **属性**：`nb.theta_` 等同于 LDA 的 `means_`（均值）。

## 5. 混淆矩阵 (Confusion Matrix)

ISLP 库提供了一个好用的工具：

```python
from ISLP import confusion_table
print(confusion_table(predicted_labels, true_labels))
```

- **准确率计算**：`np.mean(predicted_labels == true_labels)`。