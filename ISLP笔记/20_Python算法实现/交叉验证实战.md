---
tags:
  - Python
  - ç®—æ³•å®ç°
  - äº¤å‰éªŒè¯
  - æ¨¡å‹é€‰æ‹©
  - sklearn
aliases:
  - Cross-Validation Implementation
  - CVå®æˆ˜
---

# äº¤å‰éªŒè¯å®æˆ˜ (Cross-Validation Implementation)

## 0. æ ¸å¿ƒå·¥å…·åº“

æœ¬æ¬¡å®éªŒä¸»è¦ä¾èµ– `sklearn` çš„æ¨¡å‹é€‰æ‹©æ¨¡å—å’Œ `statsmodels` çš„ç»Ÿè®¡æ¨æ–­èƒ½åŠ›ã€‚
* **åŒ…è£…å™¨**ï¼š`ISLP.models.sklearn_sm` æ˜¯å…³é”®ï¼Œå®ƒå°† `statsmodels` åŒ…è£…æˆ `sklearn` å…¼å®¹çš„ä¼°è®¡å™¨ï¼Œæ‰“é€šäº†ä¸¤ä¸ªåº“çš„å£å’ã€‚

```python
from sklearn.model_selection import train_test_split, cross_validate, KFold, ShuffleSplit
from ISLP.models import sklearn_sm
import statsmodels.api as sm
import numpy as np
```

## 1. éªŒè¯é›†æ–¹æ³• (The Validation Set Approach)

å¯¹åº”ç†è®ºï¼š[[éªŒè¯é›†æ–¹æ³•]]

### åŸºç¡€å®ç°

æœ€åŸå§‹çš„â€œåˆ‡ä¸€åˆ€â€æ³•ã€‚

```python
# 1. åˆ‡åˆ†æ•°æ® (50% è®­ç»ƒ, 50% éªŒè¯)
Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=0)

# 2. è®­ç»ƒæ¨¡å‹ (åªç”¨è®­ç»ƒé›†)
hp_mm = MS(['horsepower'])
X_train = hp_mm.fit_transform(Auto_train)
y_train = Auto_train['mpg']
model = sm.OLS(y_train, X_train).fit()

# 3. è¯„ä¼°æ¨¡å‹ (ç”¨éªŒè¯é›†æµ‹ MSE)
X_valid = hp_mm.transform(Auto_valid)
valid_pred = model.predict(X_valid)
mse = np.mean((Auto_valid['mpg'] - valid_pred)**2)
print(f"Validation MSE: {mse}")
```

- **ç¼ºç‚¹æ¼”ç¤º**ï¼šå½“ä½ ä¿®æ”¹ `random_state=0` ä¸º `random_state=1` æ—¶ï¼ŒMSE ä¼šå‰§çƒˆè·³åŠ¨ï¼ˆå¦‚ä» 24.5 å˜åˆ° 28.1ï¼‰ã€‚è¿™è¯æ˜äº†éªŒè¯é›†æ–¹æ³•çš„é«˜æ–¹å·®ç‰¹æ€§ã€‚

### é«˜çº§å®ç°ï¼šä½¿ç”¨ `ShuffleSplit`

è¿™æ˜¯ç”¨ `cross_validate` æ¡†æ¶æ¥æ¨¡æ‹ŸéªŒè¯é›†æ–¹æ³•ï¼Œè™½ç„¶æ˜¯â€œç‰›åˆ€æ€é¸¡â€ï¼Œä½†ä¿è¯äº†æ¥å£ç»Ÿä¸€ã€‚

```python
# n_splits=1 æ„å‘³ç€åªåˆ‡ä¸€æ¬¡ï¼ˆå³éªŒè¯é›†æ³•ï¼‰
validation = ShuffleSplit(n_splits=1, test_size=196, random_state=0)
results = cross_validate(hp_model, X, Y, cv=validation)
```

---

## 2. ç•™ä¸€æ³•äº¤å‰éªŒè¯ (LOOCV)

å¯¹åº”ç†è®ºï¼š[[ç•™ä¸€æ³•äº¤å‰éªŒè¯]]

LOOCV æ˜¯ $K=n$ çš„ç‰¹ä¾‹ã€‚åœ¨ `sklearn` ä¸­ï¼Œåªéœ€å°† `cv` å‚æ•°è®¾ä¸ºæ ·æœ¬æ€»æ•°å³å¯ã€‚

```python
# cv=Auto.shape[0] å³ 392ï¼Œè§¦å‘ LOOCV
cv_results = cross_validate(hp_model, X, Y, cv=Auto.shape[0])
loocv_mse = np.mean(cv_results['test_score'])
```

- **å®æˆ˜æ´å¯Ÿ**ï¼š
  - **è€—æ—¶**ï¼šéœ€è¦è®­ç»ƒ 392 æ¬¡æ¨¡å‹ã€‚
  - **ç»“æœ**ï¼š`loocv_mse` æ˜¯ä¸€ä¸ªç¡®å®šå€¼ï¼ˆæ— éšæœºæ€§ï¼‰ï¼Œçº¦ä¸º 24.23ã€‚è¿™æ˜¯è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„åŸºå‡†ã€‚

---

## 3. KæŠ˜äº¤å‰éªŒè¯ (K-Fold CV)

å¯¹åº”ç†è®ºï¼š[[KæŠ˜äº¤å‰éªŒè¯]]

è¿™æ˜¯ç¾èµ›ä¸­æœ€å¸¸ç”¨çš„æ–¹æ¡ˆï¼Œå…¼é¡¾æ•ˆç‡ä¸ç¨³å®šæ€§ã€‚

### æ ¸å¿ƒå¯¹è±¡ï¼š`KFold`

```python
# å®šä¹‰åˆ‡åˆ†ç­–ç•¥ï¼š10æŠ˜ï¼Œå¿…é¡»æ´—ç‰Œ (shuffle=True)
cv = KFold(n_splits=10, shuffle=True, random_state=0)

# æ‰§è¡Œäº¤å‰éªŒè¯
cv_results = cross_validate(hp_model, X, Y, cv=cv)
kfold_mse = np.mean(cv_results['test_score'])
```

- **å…³é”®å‚æ•°**ï¼š`shuffle=True` è‡³å…³é‡è¦ï¼å¦‚æœæ•°æ®æ˜¯æŒ‰æ—¶é—´æˆ–IDæ’åºçš„ï¼Œä¸æ´—ç‰Œä¼šå¯¼è‡´æ¯ä¸€æŠ˜çš„æ•°æ®åˆ†å¸ƒæä¸å‡åŒ€ï¼ˆBiasï¼‰ã€‚

---

## 4. æ¨¡å‹é€‰æ‹©ï¼šå¤šé¡¹å¼å›å½’çš„â€œé€‰ç§€â€

æˆ‘ä»¬ä½¿ç”¨ CV æ¥å†³å®šæœ€ä½³çš„å¤šé¡¹å¼é˜¶æ•° ($d$)ã€‚

### ç¼–ç¨‹æ‰‹å¿…æ€æŠ€ï¼š`np.power.outer`

åœ¨å¾ªç¯ä¸­æ„å»ºå¤šé¡¹å¼ç‰¹å¾çŸ©é˜µæ—¶ï¼Œä¸å…¶æ‰‹åŠ¨æ‹¼æ¥åˆ—ï¼Œä¸å¦‚ä½¿ç”¨ NumPy çš„å¹¿æ’­æœºåˆ¶ã€‚

```python
H = np.array(Auto['horsepower'])
cv_error = np.zeros(5)

for i, d in enumerate(range(1, 6)):
    # è‡ªåŠ¨ç”Ÿæˆè®¾è®¡çŸ©é˜µï¼šåŒ…å« [1, H, H^2, ... H^d]
    # np.arange(d+1) ç”ŸæˆæŒ‡æ•° [0, 1, ..., d]
    # outer è¿ç®—è®© H çš„æ¯ä¸ªå…ƒç´ éƒ½å»åŒ¹é…æ¯ä¸ªæŒ‡æ•°
    X = np.power.outer(H, np.arange(d+1))
    
    # ä½¿ç”¨åŒä¸€ä¸ª cv å¯¹è±¡ï¼Œä¿è¯å…¬å¹³ç«äº‰ (Apples-to-Apples)
    M_CV = cross_validate(M, X, Y, cv=cv)
    cv_error[i] = np.mean(M_CV['test_score'])
```

- **enumerateæŠ€å·§**ï¼š`enumerate(range(1, 6))` åŒæ—¶æä¾›äº† ç´¢å¼• `i` (0-4) ç”¨äºå­˜ç»“æœï¼Œå’Œ é˜¶æ•° `d` (1-5) ç”¨äºå»ºæ¨¡ï¼Œé¿å…äº† `i = d-1` è¿™ç§æ˜“é”™çš„ç®—æœ¯ã€‚

---

## 5. è¿›é˜¶ï¼šè’™ç‰¹å¡æ´›äº¤å‰éªŒè¯ (Repeated CV)

ä½¿ç”¨ `ShuffleSplit` è¿›è¡Œå¤šæ¬¡éšæœºå­é‡‡æ ·ã€‚

```python
# æ¯æ¬¡éšæœºæŠ½ä¸€åŠåšæµ‹è¯•ï¼Œé‡å¤ 10 æ¬¡
validation = ShuffleSplit(n_splits=10, test_size=196, random_state=0)
results = cross_validate(hp_model, X, Y, cv=validation)

print(f"Mean MSE: {results['test_score'].mean()}") # å‡†ç¡®åº¦
print(f"Std MSE:  {results['test_score'].std()}")  # ç¨³å®šæ€§/é²æ£’æ€§
```

- **åº”ç”¨åœºæ™¯**ï¼šå‹åŠ›æµ‹è¯•ã€‚å¦‚æœ `std` å¾ˆå¤§ï¼Œè¯´æ˜æ¨¡å‹â€œçœ‹è„¸â€ï¼ˆå¯¹æ•°æ®åˆ’åˆ†æ•æ„Ÿï¼‰ï¼Œåœ¨ç¾èµ›ä¸­æ˜¯ä¸ç¨³å®šçš„é£é™©ä¿¡å·ã€‚

---

## è­¦ç¤ºï¼šæ•°æ®æ³„éœ²ä¸ Pipeline

åœ¨åš CV æ—¶ï¼Œ**ä¸¥ç¦**å…ˆåœ¨å¤–éƒ¨å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œ `fit_transform`ï¼ˆå¦‚æ ‡å‡†åŒ–ï¼‰ã€‚

- **é”™è¯¯åšæ³•**ï¼š`StandardScaler().fit(All_Data)` $\to$ ä¿¡æ¯æ³„éœ²ã€‚
- **æ­£ç¡®åšæ³•**ï¼šä½¿ç”¨ `sklearn.pipeline.Pipeline`ã€‚

```python
pipe = Pipeline([
    ('scaler', StandardScaler()), # æ¯æ¬¡åªåœ¨è®­ç»ƒæŠ˜ä¸Š fit
    ('model', sklearn_sm(sm.OLS))
])
cross_validate(pipe, X, Y, cv=10)
```

è¿™ä¿è¯äº†â€œè®­ç»ƒé›† fitï¼Œæµ‹è¯•é›† transformâ€ï¼Œæ˜¯æ•°æ®ç§‘å­¦ç«èµ›çš„é“å¾‹ã€‚

---
## ğŸ”— ç†è®ºå›æº¯
- **æ ¸å¿ƒç†è®º**ï¼š[[éªŒè¯é›†æ–¹æ³•]]ã€[[ç•™ä¸€æ³•äº¤å‰éªŒè¯]]ã€[[KæŠ˜äº¤å‰éªŒè¯]]
- **ç›¸å…³æ¦‚å¿µ**ï¼š[[åå·®-æ–¹å·®æƒè¡¡]]