---
tags:
  - Python实战
  - 特征选择
  - 逐步回归
  - 模型评估
aliases:
  - Subset Selection Lab
  - Stepwise Selection
  - Cp Statistic
---

# 子集选择与逐步回归实战 (Subset Selection)

## 0. 编程手核心直觉

* **暴力美学 vs 贪心策略**：
    * **最优子集 (Best Subset)**：遍历 $2^p$ 种组合，寻找理论最优。计算量大，适合 $p$ 较小时。
    * **逐步回归 (Stepwise)**：贪心算法（只看眼前最好的一步）。计算快，适合 $p$ 较大时。
* **评分标准**：训练误差 (RSS/MSE) 一定会随着变量增加而降低，因此**不能**用 RSS 选模型。必须使用 $C_p, AIC, BIC$ 或 **调整 $R^2$**。

**🔗 理论回溯**：[[子集选择法]] | [[模型选择指标]]

---

## 1. 逐步回归 (Forward Stepwise)

### 1.1 策略制定 (The Strategy)
`ISLP` 库将“选择策略”对象化了，这非常符合面向对象编程思维。

```python
from ISLP.models import ModelSpec as MS
from ISLP.models import Stepwise

# 1. 准备设计矩阵 (不含 Salary 列)
design = MS(Hitters.columns.drop('Salary')).fit(Hitters)

# 2. 制定战术 (Strategy)
# direction='forward': 从零模型开始，一个加。
# max_terms: 允许加到满为止。
strategy = Stepwise.first_peak(
    design,
    direction='forward',
    max_terms=len(design.terms)
)
```

### 1.2 自定义评分标准 ($C_p$)

`sklearn` 默认没有 $C_p$ 指标，我们需要手动注入“编程手”的数学理解。

- **公式**：$C_p = \frac{1}{n} (RSS + 2d\hat{\sigma}^2)$
- **实现技巧**：使用 `functools.partial` 冻结全模型的方差 $\sigma^2$。

Python

```python
from functools import partial
from statsmodels.api import OLS

# 1. 计算全模型的 sigma^2 (作为噪声水平的基准)
Y = np.array(Hitters['Salary'])
X_full = design.transform(Hitters)
full_model = OLS(Y, X_full).fit()
sigma2 = full_model.scale  # 获取全模型方差

# 2. 定义 Cp 计算函数 (注意 sklearn score 通常是越大越好，所以取负值)
def nCp(sigma2, estimator, X, y):
    n, p = X.shape
    y_pred = estimator.predict(X)
    RSS = np.sum((y - y_pred)**2)
    return -(RSS + 2 * p * sigma2) # 负 Cp

# 3. 冻结参数，生成专用打分器
neg_Cp = partial(nCp, sigma2)
```

### 1.3 战术执行与可视化

这里展示了如何画出那张经典的“训练误差 vs 变量个数”图。

Python

```python
from ISLP.models import sklearn_selected, sklearn_selection_path

# 1. 执行选择，保留全路径
strategy_path = Stepwise.fixed_steps(design, len(design.terms), direction='forward')
full_path = sklearn_selection_path(OLS, strategy_path)
full_path.fit(Hitters, Y)

# 2. 预测并计算训练 MSE (In-sample)
Yhat_in = full_path.predict(Hitters) # 形状 (n_samples, n_steps)
# 广播机制：(n, 20) - (n, 1)
insample_mse = ((Yhat_in - Y[:, None])**2).mean(0) 
```

---

## 2. 最优子集选择 (Best Subset)

**⚠️ 算力警告**：此方法使用 `l0bnb` 库的分支定界算法，虽然比暴力穷举快，但 $p>30$ 时仍需谨慎。

Python

```python
from l0bnb import fit_path

# 1. 准备纯净矩阵 (剔除截距，l0bnb 会自动处理)
D = design.fit_transform(Hitters).drop('intercept', axis=1)
X_np = np.asarray(D)

# 2. 寻找最优路径 (Branch and Bound)
# max_nonzeros: 允许最大的非零系数个数
path = fit_path(X_np, Y, max_nonzeros=19)

# 3. 解析结果 (path 是一个列表，每个元素是某 k 值下的最佳模型)
# path[3] 代表只有 3 个非零变量时的最佳模型
print(f"k=3 时的系数: {path[3]['B']}") 
```

---

## 3. 常见陷阱

- **缺失值**：逐步回归对缺失值零容忍。代码第一步必须是 `Hitters.dropna()`。
- **指标方向**：`AIC/BIC/Cp` 都是越**小**越好，`Adj-R2` 是越**大**越好。在 `sklearn` 的 `scoring` 参数中，通常需要**取负号**来适配“最大化”逻辑。